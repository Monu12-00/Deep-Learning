{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79fb6d46-e163-42d8-961f-e3b98bbeae6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch)\n",
      "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /apps/cent7/anaconda/2024.02/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/mrameshk/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ef93a1-9450-4085-9a72-12833af1929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd8476b-c7b1-4e2c-9f31-9399fc6b9d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74e2f6b-7a23-4eaa-aabb-3921af79e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "latent_dim = 100\n",
    "num_classes = 10\n",
    "lr = 0.0002\n",
    "epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6a433f-bcbd-4437-9940-4591842d2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 32 * 32 * 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), 3, 32, 32)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52664163-f5e9-4522-bc6e-e829944bdbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_classes + 32 * 32 * 3, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        d_in = img.view(img.size(0), -1)\n",
    "        dis_input = torch.cat((d_in, self.label_emb(labels)), -1)\n",
    "        validity = self.model(dis_input)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052ab0d4-1349-471c-9e4f-789fcc7ed340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc625556-802f-47b7-a28a-5131cc77edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32970f3b-b1ee-459c-9db6-bfef756ea4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43d2a1fc-9ed1-434b-87e5-827eae44379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:02<00:00, 65643083.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "# Dataset and DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb195fb2-2318-40af-8d37-f9b9c32ef4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 0/782] [D loss: 0.750015] [G loss: 1.667734]\n",
      "[Epoch 0/50] [Batch 100/782] [D loss: 0.555477] [G loss: 1.561500]\n",
      "[Epoch 0/50] [Batch 200/782] [D loss: 0.510027] [G loss: 1.866041]\n",
      "[Epoch 0/50] [Batch 300/782] [D loss: 0.696911] [G loss: 1.489877]\n",
      "[Epoch 0/50] [Batch 400/782] [D loss: 0.623170] [G loss: 1.563631]\n",
      "[Epoch 0/50] [Batch 500/782] [D loss: 0.594169] [G loss: 1.497278]\n",
      "[Epoch 0/50] [Batch 600/782] [D loss: 0.582114] [G loss: 1.755124]\n",
      "[Epoch 0/50] [Batch 700/782] [D loss: 0.558877] [G loss: 1.682624]\n",
      "[Epoch 1/50] [Batch 0/782] [D loss: 0.551541] [G loss: 1.738785]\n",
      "[Epoch 1/50] [Batch 100/782] [D loss: 0.616140] [G loss: 1.585098]\n",
      "[Epoch 1/50] [Batch 200/782] [D loss: 0.555593] [G loss: 1.671649]\n",
      "[Epoch 1/50] [Batch 300/782] [D loss: 0.611367] [G loss: 1.342386]\n",
      "[Epoch 1/50] [Batch 400/782] [D loss: 0.609334] [G loss: 1.654291]\n",
      "[Epoch 1/50] [Batch 500/782] [D loss: 0.598365] [G loss: 1.259780]\n",
      "[Epoch 1/50] [Batch 600/782] [D loss: 0.563802] [G loss: 1.764005]\n",
      "[Epoch 1/50] [Batch 700/782] [D loss: 0.571646] [G loss: 1.264813]\n",
      "[Epoch 2/50] [Batch 0/782] [D loss: 0.704866] [G loss: 1.383064]\n",
      "[Epoch 2/50] [Batch 100/782] [D loss: 0.632163] [G loss: 1.298564]\n",
      "[Epoch 2/50] [Batch 200/782] [D loss: 0.556060] [G loss: 1.832680]\n",
      "[Epoch 2/50] [Batch 300/782] [D loss: 0.617233] [G loss: 1.541927]\n",
      "[Epoch 2/50] [Batch 400/782] [D loss: 0.617828] [G loss: 1.221237]\n",
      "[Epoch 2/50] [Batch 500/782] [D loss: 0.542511] [G loss: 1.801296]\n",
      "[Epoch 2/50] [Batch 600/782] [D loss: 0.600071] [G loss: 1.744211]\n",
      "[Epoch 2/50] [Batch 700/782] [D loss: 0.427952] [G loss: 2.425928]\n",
      "[Epoch 3/50] [Batch 0/782] [D loss: 0.618377] [G loss: 1.389159]\n",
      "[Epoch 3/50] [Batch 100/782] [D loss: 0.530313] [G loss: 1.869791]\n",
      "[Epoch 3/50] [Batch 200/782] [D loss: 0.561142] [G loss: 1.893552]\n",
      "[Epoch 3/50] [Batch 300/782] [D loss: 0.524449] [G loss: 1.951953]\n",
      "[Epoch 3/50] [Batch 400/782] [D loss: 0.565264] [G loss: 2.058805]\n",
      "[Epoch 3/50] [Batch 500/782] [D loss: 0.608362] [G loss: 2.300961]\n",
      "[Epoch 3/50] [Batch 600/782] [D loss: 0.604137] [G loss: 1.635131]\n",
      "[Epoch 3/50] [Batch 700/782] [D loss: 0.518095] [G loss: 2.353767]\n",
      "[Epoch 4/50] [Batch 0/782] [D loss: 0.679097] [G loss: 2.082722]\n",
      "[Epoch 4/50] [Batch 100/782] [D loss: 0.554969] [G loss: 1.619375]\n",
      "[Epoch 4/50] [Batch 200/782] [D loss: 0.554605] [G loss: 1.907548]\n",
      "[Epoch 4/50] [Batch 300/782] [D loss: 0.633437] [G loss: 2.146149]\n",
      "[Epoch 4/50] [Batch 400/782] [D loss: 0.483500] [G loss: 1.999293]\n",
      "[Epoch 4/50] [Batch 500/782] [D loss: 0.655594] [G loss: 2.722589]\n",
      "[Epoch 4/50] [Batch 600/782] [D loss: 0.762108] [G loss: 1.444422]\n",
      "[Epoch 4/50] [Batch 700/782] [D loss: 0.667539] [G loss: 1.668752]\n",
      "[Epoch 5/50] [Batch 0/782] [D loss: 0.681231] [G loss: 2.423189]\n",
      "[Epoch 5/50] [Batch 100/782] [D loss: 0.565329] [G loss: 1.885307]\n",
      "[Epoch 5/50] [Batch 200/782] [D loss: 0.461129] [G loss: 2.275371]\n",
      "[Epoch 5/50] [Batch 300/782] [D loss: 0.464253] [G loss: 1.987121]\n",
      "[Epoch 5/50] [Batch 400/782] [D loss: 0.525415] [G loss: 1.713815]\n",
      "[Epoch 5/50] [Batch 500/782] [D loss: 0.466601] [G loss: 1.939709]\n",
      "[Epoch 5/50] [Batch 600/782] [D loss: 0.572790] [G loss: 1.461451]\n",
      "[Epoch 5/50] [Batch 700/782] [D loss: 0.620074] [G loss: 1.349845]\n",
      "[Epoch 6/50] [Batch 0/782] [D loss: 0.667268] [G loss: 1.394192]\n",
      "[Epoch 6/50] [Batch 100/782] [D loss: 0.590293] [G loss: 1.153230]\n",
      "[Epoch 6/50] [Batch 200/782] [D loss: 0.648161] [G loss: 1.029251]\n",
      "[Epoch 6/50] [Batch 300/782] [D loss: 0.590122] [G loss: 1.166012]\n",
      "[Epoch 6/50] [Batch 400/782] [D loss: 0.621504] [G loss: 0.957259]\n",
      "[Epoch 6/50] [Batch 500/782] [D loss: 0.703511] [G loss: 1.028845]\n",
      "[Epoch 6/50] [Batch 600/782] [D loss: 0.638077] [G loss: 1.170159]\n",
      "[Epoch 6/50] [Batch 700/782] [D loss: 0.645793] [G loss: 1.019828]\n",
      "[Epoch 7/50] [Batch 0/782] [D loss: 0.697320] [G loss: 0.936089]\n",
      "[Epoch 7/50] [Batch 100/782] [D loss: 0.604728] [G loss: 1.310617]\n",
      "[Epoch 7/50] [Batch 200/782] [D loss: 0.646203] [G loss: 1.166340]\n",
      "[Epoch 7/50] [Batch 300/782] [D loss: 0.612553] [G loss: 1.329019]\n",
      "[Epoch 7/50] [Batch 400/782] [D loss: 0.654299] [G loss: 1.095458]\n",
      "[Epoch 7/50] [Batch 500/782] [D loss: 0.714831] [G loss: 1.369450]\n",
      "[Epoch 7/50] [Batch 600/782] [D loss: 0.585275] [G loss: 1.245254]\n",
      "[Epoch 7/50] [Batch 700/782] [D loss: 0.595694] [G loss: 1.195305]\n",
      "[Epoch 8/50] [Batch 0/782] [D loss: 0.712375] [G loss: 0.982655]\n",
      "[Epoch 8/50] [Batch 100/782] [D loss: 0.650666] [G loss: 0.990349]\n",
      "[Epoch 8/50] [Batch 200/782] [D loss: 0.665032] [G loss: 1.219088]\n",
      "[Epoch 8/50] [Batch 300/782] [D loss: 0.645686] [G loss: 0.982304]\n",
      "[Epoch 8/50] [Batch 400/782] [D loss: 0.714146] [G loss: 0.884469]\n",
      "[Epoch 8/50] [Batch 500/782] [D loss: 0.658399] [G loss: 1.138073]\n",
      "[Epoch 8/50] [Batch 600/782] [D loss: 0.680266] [G loss: 1.105584]\n",
      "[Epoch 8/50] [Batch 700/782] [D loss: 0.618721] [G loss: 1.144744]\n",
      "[Epoch 9/50] [Batch 0/782] [D loss: 0.650771] [G loss: 1.025038]\n",
      "[Epoch 9/50] [Batch 100/782] [D loss: 0.677486] [G loss: 1.116931]\n",
      "[Epoch 9/50] [Batch 200/782] [D loss: 0.636051] [G loss: 1.082125]\n",
      "[Epoch 9/50] [Batch 300/782] [D loss: 0.663489] [G loss: 0.903021]\n",
      "[Epoch 9/50] [Batch 400/782] [D loss: 0.697511] [G loss: 0.994999]\n",
      "[Epoch 9/50] [Batch 500/782] [D loss: 0.610940] [G loss: 1.004425]\n",
      "[Epoch 9/50] [Batch 600/782] [D loss: 0.651355] [G loss: 1.044124]\n",
      "[Epoch 9/50] [Batch 700/782] [D loss: 0.727955] [G loss: 0.850684]\n",
      "[Epoch 10/50] [Batch 0/782] [D loss: 0.652241] [G loss: 0.958793]\n",
      "[Epoch 10/50] [Batch 100/782] [D loss: 0.667907] [G loss: 0.969221]\n",
      "[Epoch 10/50] [Batch 200/782] [D loss: 0.623465] [G loss: 1.264325]\n",
      "[Epoch 10/50] [Batch 300/782] [D loss: 0.620786] [G loss: 1.045753]\n",
      "[Epoch 10/50] [Batch 400/782] [D loss: 0.704958] [G loss: 0.922039]\n",
      "[Epoch 10/50] [Batch 500/782] [D loss: 0.680152] [G loss: 0.852118]\n",
      "[Epoch 10/50] [Batch 600/782] [D loss: 0.707227] [G loss: 0.882522]\n",
      "[Epoch 10/50] [Batch 700/782] [D loss: 0.659478] [G loss: 0.865244]\n",
      "[Epoch 11/50] [Batch 0/782] [D loss: 0.646122] [G loss: 0.911832]\n",
      "[Epoch 11/50] [Batch 100/782] [D loss: 0.721256] [G loss: 0.777547]\n",
      "[Epoch 11/50] [Batch 200/782] [D loss: 0.694227] [G loss: 0.834688]\n",
      "[Epoch 11/50] [Batch 300/782] [D loss: 0.691512] [G loss: 0.876629]\n",
      "[Epoch 11/50] [Batch 400/782] [D loss: 0.678131] [G loss: 0.823061]\n",
      "[Epoch 11/50] [Batch 500/782] [D loss: 0.691463] [G loss: 0.853354]\n",
      "[Epoch 11/50] [Batch 600/782] [D loss: 0.666847] [G loss: 0.772315]\n",
      "[Epoch 11/50] [Batch 700/782] [D loss: 0.679129] [G loss: 0.858768]\n",
      "[Epoch 12/50] [Batch 0/782] [D loss: 0.685024] [G loss: 0.858246]\n",
      "[Epoch 12/50] [Batch 100/782] [D loss: 0.673975] [G loss: 0.893887]\n",
      "[Epoch 12/50] [Batch 200/782] [D loss: 0.654066] [G loss: 0.870167]\n",
      "[Epoch 12/50] [Batch 300/782] [D loss: 0.660898] [G loss: 0.812664]\n",
      "[Epoch 12/50] [Batch 400/782] [D loss: 0.715492] [G loss: 0.777738]\n",
      "[Epoch 12/50] [Batch 500/782] [D loss: 0.654125] [G loss: 0.751686]\n",
      "[Epoch 12/50] [Batch 600/782] [D loss: 0.671734] [G loss: 0.773960]\n",
      "[Epoch 12/50] [Batch 700/782] [D loss: 0.706396] [G loss: 0.780408]\n",
      "[Epoch 13/50] [Batch 0/782] [D loss: 0.692865] [G loss: 0.836654]\n",
      "[Epoch 13/50] [Batch 100/782] [D loss: 0.675023] [G loss: 0.825822]\n",
      "[Epoch 13/50] [Batch 200/782] [D loss: 0.703889] [G loss: 0.846669]\n",
      "[Epoch 13/50] [Batch 300/782] [D loss: 0.663083] [G loss: 0.796183]\n",
      "[Epoch 13/50] [Batch 400/782] [D loss: 0.675949] [G loss: 0.809780]\n",
      "[Epoch 13/50] [Batch 500/782] [D loss: 0.684252] [G loss: 0.793967]\n",
      "[Epoch 13/50] [Batch 600/782] [D loss: 0.654678] [G loss: 0.848443]\n",
      "[Epoch 13/50] [Batch 700/782] [D loss: 0.679644] [G loss: 0.767145]\n",
      "[Epoch 14/50] [Batch 0/782] [D loss: 0.692770] [G loss: 0.795590]\n",
      "[Epoch 14/50] [Batch 100/782] [D loss: 0.660331] [G loss: 0.871875]\n",
      "[Epoch 14/50] [Batch 200/782] [D loss: 0.659996] [G loss: 0.869312]\n",
      "[Epoch 14/50] [Batch 300/782] [D loss: 0.688022] [G loss: 0.795331]\n",
      "[Epoch 14/50] [Batch 400/782] [D loss: 0.673355] [G loss: 0.786720]\n",
      "[Epoch 14/50] [Batch 500/782] [D loss: 0.715658] [G loss: 0.756444]\n",
      "[Epoch 14/50] [Batch 600/782] [D loss: 0.663396] [G loss: 0.944357]\n",
      "[Epoch 14/50] [Batch 700/782] [D loss: 0.681304] [G loss: 0.813094]\n",
      "[Epoch 15/50] [Batch 0/782] [D loss: 0.688443] [G loss: 0.912656]\n",
      "[Epoch 15/50] [Batch 100/782] [D loss: 0.692598] [G loss: 0.769593]\n",
      "[Epoch 15/50] [Batch 200/782] [D loss: 0.675783] [G loss: 0.812920]\n",
      "[Epoch 15/50] [Batch 300/782] [D loss: 0.712018] [G loss: 0.837188]\n",
      "[Epoch 15/50] [Batch 400/782] [D loss: 0.664084] [G loss: 0.794931]\n",
      "[Epoch 15/50] [Batch 500/782] [D loss: 0.664402] [G loss: 0.821496]\n",
      "[Epoch 15/50] [Batch 600/782] [D loss: 0.705400] [G loss: 0.755808]\n",
      "[Epoch 15/50] [Batch 700/782] [D loss: 0.645303] [G loss: 0.826428]\n",
      "[Epoch 16/50] [Batch 0/782] [D loss: 0.680378] [G loss: 0.799401]\n",
      "[Epoch 16/50] [Batch 100/782] [D loss: 0.693223] [G loss: 0.893576]\n",
      "[Epoch 16/50] [Batch 200/782] [D loss: 0.698149] [G loss: 0.800248]\n",
      "[Epoch 16/50] [Batch 300/782] [D loss: 0.648861] [G loss: 0.825133]\n",
      "[Epoch 16/50] [Batch 400/782] [D loss: 0.687583] [G loss: 0.824654]\n",
      "[Epoch 16/50] [Batch 500/782] [D loss: 0.701140] [G loss: 0.760161]\n",
      "[Epoch 16/50] [Batch 600/782] [D loss: 0.672143] [G loss: 0.877026]\n",
      "[Epoch 16/50] [Batch 700/782] [D loss: 0.718452] [G loss: 0.798225]\n",
      "[Epoch 17/50] [Batch 0/782] [D loss: 0.675169] [G loss: 0.796297]\n",
      "[Epoch 17/50] [Batch 100/782] [D loss: 0.683146] [G loss: 0.803021]\n",
      "[Epoch 17/50] [Batch 200/782] [D loss: 0.699442] [G loss: 0.871600]\n",
      "[Epoch 17/50] [Batch 300/782] [D loss: 0.687385] [G loss: 0.784037]\n",
      "[Epoch 17/50] [Batch 400/782] [D loss: 0.712935] [G loss: 0.774076]\n",
      "[Epoch 17/50] [Batch 500/782] [D loss: 0.692210] [G loss: 0.834062]\n",
      "[Epoch 17/50] [Batch 600/782] [D loss: 0.679943] [G loss: 0.737140]\n",
      "[Epoch 17/50] [Batch 700/782] [D loss: 0.721331] [G loss: 0.812703]\n",
      "[Epoch 18/50] [Batch 0/782] [D loss: 0.693413] [G loss: 0.756666]\n",
      "[Epoch 18/50] [Batch 100/782] [D loss: 0.692033] [G loss: 0.781691]\n",
      "[Epoch 18/50] [Batch 200/782] [D loss: 0.704004] [G loss: 0.816305]\n",
      "[Epoch 18/50] [Batch 300/782] [D loss: 0.690399] [G loss: 0.782219]\n",
      "[Epoch 18/50] [Batch 400/782] [D loss: 0.679836] [G loss: 0.776628]\n",
      "[Epoch 18/50] [Batch 500/782] [D loss: 0.682950] [G loss: 0.815378]\n",
      "[Epoch 18/50] [Batch 600/782] [D loss: 0.715228] [G loss: 0.786550]\n",
      "[Epoch 18/50] [Batch 700/782] [D loss: 0.695663] [G loss: 0.766876]\n",
      "[Epoch 19/50] [Batch 0/782] [D loss: 0.679950] [G loss: 0.855545]\n",
      "[Epoch 19/50] [Batch 100/782] [D loss: 0.676634] [G loss: 0.756986]\n",
      "[Epoch 19/50] [Batch 200/782] [D loss: 0.652756] [G loss: 0.797408]\n",
      "[Epoch 19/50] [Batch 300/782] [D loss: 0.688483] [G loss: 0.810089]\n",
      "[Epoch 19/50] [Batch 400/782] [D loss: 0.691354] [G loss: 0.854389]\n",
      "[Epoch 19/50] [Batch 500/782] [D loss: 0.683084] [G loss: 0.803959]\n",
      "[Epoch 19/50] [Batch 600/782] [D loss: 0.697937] [G loss: 0.789499]\n",
      "[Epoch 19/50] [Batch 700/782] [D loss: 0.694950] [G loss: 0.755874]\n",
      "[Epoch 20/50] [Batch 0/782] [D loss: 0.645806] [G loss: 0.934246]\n",
      "[Epoch 20/50] [Batch 100/782] [D loss: 0.671301] [G loss: 0.868048]\n",
      "[Epoch 20/50] [Batch 200/782] [D loss: 0.697032] [G loss: 0.784656]\n",
      "[Epoch 20/50] [Batch 300/782] [D loss: 0.706334] [G loss: 0.822368]\n",
      "[Epoch 20/50] [Batch 400/782] [D loss: 0.695268] [G loss: 0.807276]\n",
      "[Epoch 20/50] [Batch 500/782] [D loss: 0.706021] [G loss: 0.780145]\n",
      "[Epoch 20/50] [Batch 600/782] [D loss: 0.713263] [G loss: 0.785073]\n",
      "[Epoch 20/50] [Batch 700/782] [D loss: 0.661306] [G loss: 0.870092]\n",
      "[Epoch 21/50] [Batch 0/782] [D loss: 0.685481] [G loss: 0.832116]\n",
      "[Epoch 21/50] [Batch 100/782] [D loss: 0.676796] [G loss: 0.854068]\n",
      "[Epoch 21/50] [Batch 200/782] [D loss: 0.707756] [G loss: 0.762057]\n",
      "[Epoch 21/50] [Batch 300/782] [D loss: 0.678866] [G loss: 0.827282]\n",
      "[Epoch 21/50] [Batch 400/782] [D loss: 0.691342] [G loss: 0.723871]\n",
      "[Epoch 21/50] [Batch 500/782] [D loss: 0.673410] [G loss: 0.773288]\n",
      "[Epoch 21/50] [Batch 600/782] [D loss: 0.709202] [G loss: 0.745030]\n",
      "[Epoch 21/50] [Batch 700/782] [D loss: 0.678365] [G loss: 0.752336]\n",
      "[Epoch 22/50] [Batch 0/782] [D loss: 0.695860] [G loss: 0.860247]\n",
      "[Epoch 22/50] [Batch 100/782] [D loss: 0.709389] [G loss: 0.756158]\n",
      "[Epoch 22/50] [Batch 200/782] [D loss: 0.686439] [G loss: 0.789264]\n",
      "[Epoch 22/50] [Batch 300/782] [D loss: 0.715369] [G loss: 0.762607]\n",
      "[Epoch 22/50] [Batch 400/782] [D loss: 0.700312] [G loss: 0.741966]\n",
      "[Epoch 22/50] [Batch 500/782] [D loss: 0.702195] [G loss: 0.761305]\n",
      "[Epoch 22/50] [Batch 600/782] [D loss: 0.687492] [G loss: 0.811462]\n",
      "[Epoch 22/50] [Batch 700/782] [D loss: 0.712721] [G loss: 0.745674]\n",
      "[Epoch 23/50] [Batch 0/782] [D loss: 0.698869] [G loss: 0.679749]\n",
      "[Epoch 23/50] [Batch 100/782] [D loss: 0.698148] [G loss: 0.726239]\n",
      "[Epoch 23/50] [Batch 200/782] [D loss: 0.698444] [G loss: 0.746377]\n",
      "[Epoch 23/50] [Batch 300/782] [D loss: 0.690662] [G loss: 0.787041]\n",
      "[Epoch 23/50] [Batch 400/782] [D loss: 0.673052] [G loss: 0.763818]\n",
      "[Epoch 23/50] [Batch 500/782] [D loss: 0.681514] [G loss: 0.777458]\n",
      "[Epoch 23/50] [Batch 600/782] [D loss: 0.668999] [G loss: 0.831519]\n",
      "[Epoch 23/50] [Batch 700/782] [D loss: 0.701973] [G loss: 0.724867]\n",
      "[Epoch 24/50] [Batch 0/782] [D loss: 0.686942] [G loss: 0.803953]\n",
      "[Epoch 24/50] [Batch 100/782] [D loss: 0.690462] [G loss: 0.756481]\n",
      "[Epoch 24/50] [Batch 200/782] [D loss: 0.694216] [G loss: 0.774529]\n",
      "[Epoch 24/50] [Batch 300/782] [D loss: 0.717276] [G loss: 0.761985]\n",
      "[Epoch 24/50] [Batch 400/782] [D loss: 0.686101] [G loss: 0.795589]\n",
      "[Epoch 24/50] [Batch 500/782] [D loss: 0.680150] [G loss: 0.778189]\n",
      "[Epoch 24/50] [Batch 600/782] [D loss: 0.689618] [G loss: 0.736610]\n",
      "[Epoch 24/50] [Batch 700/782] [D loss: 0.711994] [G loss: 0.739995]\n",
      "[Epoch 25/50] [Batch 0/782] [D loss: 0.686081] [G loss: 0.762523]\n",
      "[Epoch 25/50] [Batch 100/782] [D loss: 0.687993] [G loss: 0.756360]\n",
      "[Epoch 25/50] [Batch 200/782] [D loss: 0.698495] [G loss: 0.734961]\n",
      "[Epoch 25/50] [Batch 300/782] [D loss: 0.680605] [G loss: 0.776087]\n",
      "[Epoch 25/50] [Batch 400/782] [D loss: 0.688557] [G loss: 0.780614]\n",
      "[Epoch 25/50] [Batch 500/782] [D loss: 0.726579] [G loss: 0.745559]\n",
      "[Epoch 25/50] [Batch 600/782] [D loss: 0.696902] [G loss: 0.804228]\n",
      "[Epoch 25/50] [Batch 700/782] [D loss: 0.687944] [G loss: 0.805762]\n",
      "[Epoch 26/50] [Batch 0/782] [D loss: 0.701585] [G loss: 0.706981]\n",
      "[Epoch 26/50] [Batch 100/782] [D loss: 0.674603] [G loss: 0.779182]\n",
      "[Epoch 26/50] [Batch 200/782] [D loss: 0.667150] [G loss: 0.787997]\n",
      "[Epoch 26/50] [Batch 300/782] [D loss: 0.672631] [G loss: 0.792354]\n",
      "[Epoch 26/50] [Batch 400/782] [D loss: 0.681512] [G loss: 0.777976]\n",
      "[Epoch 26/50] [Batch 500/782] [D loss: 0.685217] [G loss: 0.772771]\n",
      "[Epoch 26/50] [Batch 600/782] [D loss: 0.691803] [G loss: 0.771542]\n",
      "[Epoch 26/50] [Batch 700/782] [D loss: 0.685542] [G loss: 0.769228]\n",
      "[Epoch 27/50] [Batch 0/782] [D loss: 0.696945] [G loss: 0.761771]\n",
      "[Epoch 27/50] [Batch 100/782] [D loss: 0.676624] [G loss: 0.762158]\n",
      "[Epoch 27/50] [Batch 200/782] [D loss: 0.694672] [G loss: 0.788521]\n",
      "[Epoch 27/50] [Batch 300/782] [D loss: 0.710595] [G loss: 0.768099]\n",
      "[Epoch 27/50] [Batch 400/782] [D loss: 0.681525] [G loss: 0.793380]\n",
      "[Epoch 27/50] [Batch 500/782] [D loss: 0.680137] [G loss: 0.746127]\n",
      "[Epoch 27/50] [Batch 600/782] [D loss: 0.676214] [G loss: 0.779491]\n",
      "[Epoch 27/50] [Batch 700/782] [D loss: 0.678875] [G loss: 0.818489]\n",
      "[Epoch 28/50] [Batch 0/782] [D loss: 0.671256] [G loss: 0.779923]\n",
      "[Epoch 28/50] [Batch 100/782] [D loss: 0.681214] [G loss: 0.773979]\n",
      "[Epoch 28/50] [Batch 200/782] [D loss: 0.707578] [G loss: 0.747973]\n",
      "[Epoch 28/50] [Batch 300/782] [D loss: 0.698258] [G loss: 0.730352]\n",
      "[Epoch 28/50] [Batch 400/782] [D loss: 0.672607] [G loss: 0.764403]\n",
      "[Epoch 28/50] [Batch 500/782] [D loss: 0.694436] [G loss: 0.759328]\n",
      "[Epoch 28/50] [Batch 600/782] [D loss: 0.690153] [G loss: 0.737583]\n",
      "[Epoch 28/50] [Batch 700/782] [D loss: 0.693021] [G loss: 0.789855]\n",
      "[Epoch 29/50] [Batch 0/782] [D loss: 0.697809] [G loss: 0.794182]\n",
      "[Epoch 29/50] [Batch 100/782] [D loss: 0.709771] [G loss: 0.740080]\n",
      "[Epoch 29/50] [Batch 200/782] [D loss: 0.715263] [G loss: 0.741706]\n",
      "[Epoch 29/50] [Batch 300/782] [D loss: 0.712390] [G loss: 0.735566]\n",
      "[Epoch 29/50] [Batch 400/782] [D loss: 0.700128] [G loss: 0.774426]\n",
      "[Epoch 29/50] [Batch 500/782] [D loss: 0.706822] [G loss: 0.762552]\n",
      "[Epoch 29/50] [Batch 600/782] [D loss: 0.689870] [G loss: 0.712598]\n",
      "[Epoch 29/50] [Batch 700/782] [D loss: 0.704415] [G loss: 0.766987]\n",
      "[Epoch 30/50] [Batch 0/782] [D loss: 0.680728] [G loss: 0.775427]\n",
      "[Epoch 30/50] [Batch 100/782] [D loss: 0.687190] [G loss: 0.759227]\n",
      "[Epoch 30/50] [Batch 200/782] [D loss: 0.695959] [G loss: 0.712198]\n",
      "[Epoch 30/50] [Batch 300/782] [D loss: 0.677888] [G loss: 0.778260]\n",
      "[Epoch 30/50] [Batch 400/782] [D loss: 0.674205] [G loss: 0.713406]\n",
      "[Epoch 30/50] [Batch 500/782] [D loss: 0.714538] [G loss: 0.724717]\n",
      "[Epoch 30/50] [Batch 600/782] [D loss: 0.703776] [G loss: 0.732439]\n",
      "[Epoch 30/50] [Batch 700/782] [D loss: 0.703274] [G loss: 0.742228]\n",
      "[Epoch 31/50] [Batch 0/782] [D loss: 0.693868] [G loss: 0.776515]\n",
      "[Epoch 31/50] [Batch 100/782] [D loss: 0.690995] [G loss: 0.729110]\n",
      "[Epoch 31/50] [Batch 200/782] [D loss: 0.683452] [G loss: 0.795776]\n",
      "[Epoch 31/50] [Batch 300/782] [D loss: 0.698066] [G loss: 0.742914]\n",
      "[Epoch 31/50] [Batch 400/782] [D loss: 0.683242] [G loss: 0.748662]\n",
      "[Epoch 31/50] [Batch 500/782] [D loss: 0.698908] [G loss: 0.798320]\n",
      "[Epoch 31/50] [Batch 600/782] [D loss: 0.694993] [G loss: 0.694311]\n",
      "[Epoch 31/50] [Batch 700/782] [D loss: 0.689239] [G loss: 0.760208]\n",
      "[Epoch 32/50] [Batch 0/782] [D loss: 0.695021] [G loss: 0.776351]\n",
      "[Epoch 32/50] [Batch 100/782] [D loss: 0.690221] [G loss: 0.745071]\n",
      "[Epoch 32/50] [Batch 200/782] [D loss: 0.705309] [G loss: 0.757421]\n",
      "[Epoch 32/50] [Batch 300/782] [D loss: 0.691748] [G loss: 0.742392]\n",
      "[Epoch 32/50] [Batch 400/782] [D loss: 0.692099] [G loss: 0.717346]\n",
      "[Epoch 32/50] [Batch 500/782] [D loss: 0.701286] [G loss: 0.741650]\n",
      "[Epoch 32/50] [Batch 600/782] [D loss: 0.687719] [G loss: 0.733231]\n",
      "[Epoch 32/50] [Batch 700/782] [D loss: 0.690138] [G loss: 0.743735]\n",
      "[Epoch 33/50] [Batch 0/782] [D loss: 0.695677] [G loss: 0.699851]\n",
      "[Epoch 33/50] [Batch 100/782] [D loss: 0.702021] [G loss: 0.726840]\n",
      "[Epoch 33/50] [Batch 200/782] [D loss: 0.702525] [G loss: 0.724087]\n",
      "[Epoch 33/50] [Batch 300/782] [D loss: 0.678236] [G loss: 0.709137]\n",
      "[Epoch 33/50] [Batch 400/782] [D loss: 0.716349] [G loss: 0.753103]\n",
      "[Epoch 33/50] [Batch 500/782] [D loss: 0.686874] [G loss: 0.723633]\n",
      "[Epoch 33/50] [Batch 600/782] [D loss: 0.707737] [G loss: 0.725151]\n",
      "[Epoch 33/50] [Batch 700/782] [D loss: 0.683600] [G loss: 0.734387]\n",
      "[Epoch 34/50] [Batch 0/782] [D loss: 0.684960] [G loss: 0.780217]\n",
      "[Epoch 34/50] [Batch 100/782] [D loss: 0.682271] [G loss: 0.715356]\n",
      "[Epoch 34/50] [Batch 200/782] [D loss: 0.697521] [G loss: 0.734703]\n",
      "[Epoch 34/50] [Batch 300/782] [D loss: 0.680360] [G loss: 0.748514]\n",
      "[Epoch 34/50] [Batch 400/782] [D loss: 0.691501] [G loss: 0.725276]\n",
      "[Epoch 34/50] [Batch 500/782] [D loss: 0.695247] [G loss: 0.734176]\n",
      "[Epoch 34/50] [Batch 600/782] [D loss: 0.685391] [G loss: 0.736639]\n",
      "[Epoch 34/50] [Batch 700/782] [D loss: 0.692123] [G loss: 0.739537]\n",
      "[Epoch 35/50] [Batch 0/782] [D loss: 0.687339] [G loss: 0.759832]\n",
      "[Epoch 35/50] [Batch 100/782] [D loss: 0.689939] [G loss: 0.739496]\n",
      "[Epoch 35/50] [Batch 200/782] [D loss: 0.687226] [G loss: 0.725130]\n",
      "[Epoch 35/50] [Batch 300/782] [D loss: 0.692985] [G loss: 0.720794]\n",
      "[Epoch 35/50] [Batch 400/782] [D loss: 0.693836] [G loss: 0.704938]\n",
      "[Epoch 35/50] [Batch 500/782] [D loss: 0.677313] [G loss: 0.712301]\n",
      "[Epoch 35/50] [Batch 600/782] [D loss: 0.681218] [G loss: 0.726270]\n",
      "[Epoch 35/50] [Batch 700/782] [D loss: 0.692343] [G loss: 0.698815]\n",
      "[Epoch 36/50] [Batch 0/782] [D loss: 0.705382] [G loss: 0.709313]\n",
      "[Epoch 36/50] [Batch 100/782] [D loss: 0.687104] [G loss: 0.737013]\n",
      "[Epoch 36/50] [Batch 200/782] [D loss: 0.694066] [G loss: 0.721887]\n",
      "[Epoch 36/50] [Batch 300/782] [D loss: 0.671722] [G loss: 0.727394]\n",
      "[Epoch 36/50] [Batch 400/782] [D loss: 0.690565] [G loss: 0.734779]\n",
      "[Epoch 36/50] [Batch 500/782] [D loss: 0.695601] [G loss: 0.673012]\n",
      "[Epoch 36/50] [Batch 600/782] [D loss: 0.685166] [G loss: 0.714294]\n",
      "[Epoch 36/50] [Batch 700/782] [D loss: 0.683822] [G loss: 0.754224]\n",
      "[Epoch 37/50] [Batch 0/782] [D loss: 0.672768] [G loss: 0.746958]\n",
      "[Epoch 37/50] [Batch 100/782] [D loss: 0.686005] [G loss: 0.730897]\n",
      "[Epoch 37/50] [Batch 200/782] [D loss: 0.677621] [G loss: 0.775228]\n",
      "[Epoch 37/50] [Batch 300/782] [D loss: 0.684892] [G loss: 0.720337]\n",
      "[Epoch 37/50] [Batch 400/782] [D loss: 0.690297] [G loss: 0.720449]\n",
      "[Epoch 37/50] [Batch 500/782] [D loss: 0.686047] [G loss: 0.741193]\n",
      "[Epoch 37/50] [Batch 600/782] [D loss: 0.698883] [G loss: 0.725795]\n",
      "[Epoch 37/50] [Batch 700/782] [D loss: 0.690093] [G loss: 0.751610]\n",
      "[Epoch 38/50] [Batch 0/782] [D loss: 0.687108] [G loss: 0.754500]\n",
      "[Epoch 38/50] [Batch 100/782] [D loss: 0.680124] [G loss: 0.763705]\n",
      "[Epoch 38/50] [Batch 200/782] [D loss: 0.704170] [G loss: 0.741606]\n",
      "[Epoch 38/50] [Batch 300/782] [D loss: 0.700799] [G loss: 0.702570]\n",
      "[Epoch 38/50] [Batch 400/782] [D loss: 0.690646] [G loss: 0.744013]\n",
      "[Epoch 38/50] [Batch 500/782] [D loss: 0.685483] [G loss: 0.722052]\n",
      "[Epoch 38/50] [Batch 600/782] [D loss: 0.687978] [G loss: 0.748060]\n",
      "[Epoch 38/50] [Batch 700/782] [D loss: 0.687607] [G loss: 0.719350]\n",
      "[Epoch 39/50] [Batch 0/782] [D loss: 0.687604] [G loss: 0.734453]\n",
      "[Epoch 39/50] [Batch 100/782] [D loss: 0.682126] [G loss: 0.723615]\n",
      "[Epoch 39/50] [Batch 200/782] [D loss: 0.695614] [G loss: 0.730458]\n",
      "[Epoch 39/50] [Batch 300/782] [D loss: 0.689479] [G loss: 0.731597]\n",
      "[Epoch 39/50] [Batch 400/782] [D loss: 0.688890] [G loss: 0.729789]\n",
      "[Epoch 39/50] [Batch 500/782] [D loss: 0.681597] [G loss: 0.741301]\n",
      "[Epoch 39/50] [Batch 600/782] [D loss: 0.696069] [G loss: 0.707465]\n",
      "[Epoch 39/50] [Batch 700/782] [D loss: 0.685812] [G loss: 0.736454]\n",
      "[Epoch 40/50] [Batch 0/782] [D loss: 0.694244] [G loss: 0.764813]\n",
      "[Epoch 40/50] [Batch 100/782] [D loss: 0.690306] [G loss: 0.705589]\n",
      "[Epoch 40/50] [Batch 200/782] [D loss: 0.692020] [G loss: 0.719598]\n",
      "[Epoch 40/50] [Batch 300/782] [D loss: 0.689622] [G loss: 0.703574]\n",
      "[Epoch 40/50] [Batch 400/782] [D loss: 0.693039] [G loss: 0.721514]\n",
      "[Epoch 40/50] [Batch 500/782] [D loss: 0.685071] [G loss: 0.728803]\n",
      "[Epoch 40/50] [Batch 600/782] [D loss: 0.686820] [G loss: 0.747307]\n",
      "[Epoch 40/50] [Batch 700/782] [D loss: 0.688734] [G loss: 0.747971]\n",
      "[Epoch 41/50] [Batch 0/782] [D loss: 0.703735] [G loss: 0.788639]\n",
      "[Epoch 41/50] [Batch 100/782] [D loss: 0.698549] [G loss: 0.693085]\n",
      "[Epoch 41/50] [Batch 200/782] [D loss: 0.670229] [G loss: 0.751246]\n",
      "[Epoch 41/50] [Batch 300/782] [D loss: 0.688920] [G loss: 0.747573]\n",
      "[Epoch 41/50] [Batch 400/782] [D loss: 0.691553] [G loss: 0.718628]\n",
      "[Epoch 41/50] [Batch 500/782] [D loss: 0.697520] [G loss: 0.722948]\n",
      "[Epoch 41/50] [Batch 600/782] [D loss: 0.697251] [G loss: 0.732231]\n",
      "[Epoch 41/50] [Batch 700/782] [D loss: 0.690352] [G loss: 0.728575]\n",
      "[Epoch 42/50] [Batch 0/782] [D loss: 0.689817] [G loss: 0.751853]\n",
      "[Epoch 42/50] [Batch 100/782] [D loss: 0.712834] [G loss: 0.711399]\n",
      "[Epoch 42/50] [Batch 200/782] [D loss: 0.696906] [G loss: 0.721264]\n",
      "[Epoch 42/50] [Batch 300/782] [D loss: 0.699022] [G loss: 0.714317]\n",
      "[Epoch 42/50] [Batch 400/782] [D loss: 0.691344] [G loss: 0.705321]\n",
      "[Epoch 42/50] [Batch 500/782] [D loss: 0.685284] [G loss: 0.736736]\n",
      "[Epoch 42/50] [Batch 600/782] [D loss: 0.681446] [G loss: 0.716318]\n",
      "[Epoch 42/50] [Batch 700/782] [D loss: 0.691482] [G loss: 0.731211]\n",
      "[Epoch 43/50] [Batch 0/782] [D loss: 0.691882] [G loss: 0.734448]\n",
      "[Epoch 43/50] [Batch 100/782] [D loss: 0.691869] [G loss: 0.740490]\n",
      "[Epoch 43/50] [Batch 200/782] [D loss: 0.698590] [G loss: 0.776358]\n",
      "[Epoch 43/50] [Batch 300/782] [D loss: 0.681706] [G loss: 0.729439]\n",
      "[Epoch 43/50] [Batch 400/782] [D loss: 0.697789] [G loss: 0.737189]\n",
      "[Epoch 43/50] [Batch 500/782] [D loss: 0.701464] [G loss: 0.712032]\n",
      "[Epoch 43/50] [Batch 600/782] [D loss: 0.693804] [G loss: 0.718490]\n",
      "[Epoch 43/50] [Batch 700/782] [D loss: 0.687007] [G loss: 0.737500]\n",
      "[Epoch 44/50] [Batch 0/782] [D loss: 0.690944] [G loss: 0.715918]\n",
      "[Epoch 44/50] [Batch 100/782] [D loss: 0.678649] [G loss: 0.693964]\n",
      "[Epoch 44/50] [Batch 200/782] [D loss: 0.688328] [G loss: 0.762214]\n",
      "[Epoch 44/50] [Batch 300/782] [D loss: 0.707073] [G loss: 0.706097]\n",
      "[Epoch 44/50] [Batch 400/782] [D loss: 0.689319] [G loss: 0.713670]\n",
      "[Epoch 44/50] [Batch 500/782] [D loss: 0.705221] [G loss: 0.732829]\n",
      "[Epoch 44/50] [Batch 600/782] [D loss: 0.683697] [G loss: 0.740200]\n",
      "[Epoch 44/50] [Batch 700/782] [D loss: 0.688442] [G loss: 0.726666]\n",
      "[Epoch 45/50] [Batch 0/782] [D loss: 0.680567] [G loss: 0.711095]\n",
      "[Epoch 45/50] [Batch 100/782] [D loss: 0.692407] [G loss: 0.722464]\n",
      "[Epoch 45/50] [Batch 200/782] [D loss: 0.690053] [G loss: 0.700867]\n",
      "[Epoch 45/50] [Batch 300/782] [D loss: 0.683266] [G loss: 0.705067]\n",
      "[Epoch 45/50] [Batch 400/782] [D loss: 0.701268] [G loss: 0.725524]\n",
      "[Epoch 45/50] [Batch 500/782] [D loss: 0.681016] [G loss: 0.720573]\n",
      "[Epoch 45/50] [Batch 600/782] [D loss: 0.675888] [G loss: 0.720543]\n",
      "[Epoch 45/50] [Batch 700/782] [D loss: 0.681000] [G loss: 0.716038]\n",
      "[Epoch 46/50] [Batch 0/782] [D loss: 0.699758] [G loss: 0.778906]\n",
      "[Epoch 46/50] [Batch 100/782] [D loss: 0.690534] [G loss: 0.721060]\n",
      "[Epoch 46/50] [Batch 200/782] [D loss: 0.689322] [G loss: 0.741315]\n",
      "[Epoch 46/50] [Batch 300/782] [D loss: 0.688885] [G loss: 0.737249]\n",
      "[Epoch 46/50] [Batch 400/782] [D loss: 0.691697] [G loss: 0.707922]\n",
      "[Epoch 46/50] [Batch 500/782] [D loss: 0.692796] [G loss: 0.727853]\n",
      "[Epoch 46/50] [Batch 600/782] [D loss: 0.701792] [G loss: 0.724037]\n",
      "[Epoch 46/50] [Batch 700/782] [D loss: 0.712325] [G loss: 0.670791]\n",
      "[Epoch 47/50] [Batch 0/782] [D loss: 0.700301] [G loss: 0.745290]\n",
      "[Epoch 47/50] [Batch 100/782] [D loss: 0.691759] [G loss: 0.694631]\n",
      "[Epoch 47/50] [Batch 200/782] [D loss: 0.696268] [G loss: 0.731826]\n",
      "[Epoch 47/50] [Batch 300/782] [D loss: 0.690975] [G loss: 0.710225]\n",
      "[Epoch 47/50] [Batch 400/782] [D loss: 0.688662] [G loss: 0.735096]\n",
      "[Epoch 47/50] [Batch 500/782] [D loss: 0.688994] [G loss: 0.753505]\n",
      "[Epoch 47/50] [Batch 600/782] [D loss: 0.693990] [G loss: 0.734464]\n",
      "[Epoch 47/50] [Batch 700/782] [D loss: 0.699485] [G loss: 0.745113]\n",
      "[Epoch 48/50] [Batch 0/782] [D loss: 0.691539] [G loss: 0.770368]\n",
      "[Epoch 48/50] [Batch 100/782] [D loss: 0.692077] [G loss: 0.717160]\n",
      "[Epoch 48/50] [Batch 200/782] [D loss: 0.690321] [G loss: 0.725500]\n",
      "[Epoch 48/50] [Batch 300/782] [D loss: 0.689366] [G loss: 0.697423]\n",
      "[Epoch 48/50] [Batch 400/782] [D loss: 0.699172] [G loss: 0.703461]\n",
      "[Epoch 48/50] [Batch 500/782] [D loss: 0.692275] [G loss: 0.747076]\n",
      "[Epoch 48/50] [Batch 600/782] [D loss: 0.698747] [G loss: 0.688912]\n",
      "[Epoch 48/50] [Batch 700/782] [D loss: 0.681478] [G loss: 0.809863]\n",
      "[Epoch 49/50] [Batch 0/782] [D loss: 0.666501] [G loss: 0.761285]\n",
      "[Epoch 49/50] [Batch 100/782] [D loss: 0.683881] [G loss: 0.733593]\n",
      "[Epoch 49/50] [Batch 200/782] [D loss: 0.681930] [G loss: 0.710516]\n",
      "[Epoch 49/50] [Batch 300/782] [D loss: 0.683030] [G loss: 0.717372]\n",
      "[Epoch 49/50] [Batch 400/782] [D loss: 0.683516] [G loss: 0.697104]\n",
      "[Epoch 49/50] [Batch 500/782] [D loss: 0.693486] [G loss: 0.748332]\n",
      "[Epoch 49/50] [Batch 600/782] [D loss: 0.683770] [G loss: 0.685615]\n",
      "[Epoch 49/50] [Batch 700/782] [D loss: 0.704511] [G loss: 0.719606]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(imgs.size(0), 1, device=device)\n",
    "        fake = torch.zeros(imgs.size(0), 1, device=device)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        z = torch.randn(imgs.size(0), latent_dim, device=device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, labels)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs, labels), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), labels), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs, labels), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print progress\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "    # Save generated images\n",
    "    if epoch % 10 == 0:\n",
    "        save_image(gen_imgs.data[:25], \"images/%d.png\" % epoch, nrow=5, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36037f3a-f996-4121-a579-f53a1ff9e888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
